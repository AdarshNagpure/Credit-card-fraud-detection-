{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be386405-58c4-4217-9982-dda01f6f7f6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bab4d54f-00bc-400c-91e6-2e45f2894433",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fdb5766-53d9-482a-845c-d74c800f6155",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Software\\New folder\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Version of TensorFlow : 2.15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.metrics import BinaryAccuracy, Precision, Recall\n",
    "\n",
    "print(f\"Version of TensorFlow : {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dba97064-67b7-4d64-9210-08931bddeb96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"creditcard_2023.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fd6908a-15d0-46e5-b8b1-96c21440631a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.260648</td>\n",
       "      <td>-0.469648</td>\n",
       "      <td>2.496266</td>\n",
       "      <td>-0.083724</td>\n",
       "      <td>0.129681</td>\n",
       "      <td>0.732898</td>\n",
       "      <td>0.519014</td>\n",
       "      <td>-0.130006</td>\n",
       "      <td>0.727159</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.110552</td>\n",
       "      <td>0.217606</td>\n",
       "      <td>-0.134794</td>\n",
       "      <td>0.165959</td>\n",
       "      <td>0.126280</td>\n",
       "      <td>-0.434824</td>\n",
       "      <td>-0.081230</td>\n",
       "      <td>-0.151045</td>\n",
       "      <td>17982.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.985100</td>\n",
       "      <td>-0.356045</td>\n",
       "      <td>0.558056</td>\n",
       "      <td>-0.429654</td>\n",
       "      <td>0.277140</td>\n",
       "      <td>0.428605</td>\n",
       "      <td>0.406466</td>\n",
       "      <td>-0.133118</td>\n",
       "      <td>0.347452</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.194936</td>\n",
       "      <td>-0.605761</td>\n",
       "      <td>0.079469</td>\n",
       "      <td>-0.577395</td>\n",
       "      <td>0.190090</td>\n",
       "      <td>0.296503</td>\n",
       "      <td>-0.248052</td>\n",
       "      <td>-0.064512</td>\n",
       "      <td>6531.37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.260272</td>\n",
       "      <td>-0.949385</td>\n",
       "      <td>1.728538</td>\n",
       "      <td>-0.457986</td>\n",
       "      <td>0.074062</td>\n",
       "      <td>1.419481</td>\n",
       "      <td>0.743511</td>\n",
       "      <td>-0.095576</td>\n",
       "      <td>-0.261297</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005020</td>\n",
       "      <td>0.702906</td>\n",
       "      <td>0.945045</td>\n",
       "      <td>-1.154666</td>\n",
       "      <td>-0.605564</td>\n",
       "      <td>-0.312895</td>\n",
       "      <td>-0.300258</td>\n",
       "      <td>-0.244718</td>\n",
       "      <td>2513.54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.152152</td>\n",
       "      <td>-0.508959</td>\n",
       "      <td>1.746840</td>\n",
       "      <td>-1.090178</td>\n",
       "      <td>0.249486</td>\n",
       "      <td>1.143312</td>\n",
       "      <td>0.518269</td>\n",
       "      <td>-0.065130</td>\n",
       "      <td>-0.205698</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.146927</td>\n",
       "      <td>-0.038212</td>\n",
       "      <td>-0.214048</td>\n",
       "      <td>-1.893131</td>\n",
       "      <td>1.003963</td>\n",
       "      <td>-0.515950</td>\n",
       "      <td>-0.165316</td>\n",
       "      <td>0.048424</td>\n",
       "      <td>5384.44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.206820</td>\n",
       "      <td>-0.165280</td>\n",
       "      <td>1.527053</td>\n",
       "      <td>-0.448293</td>\n",
       "      <td>0.106125</td>\n",
       "      <td>0.530549</td>\n",
       "      <td>0.658849</td>\n",
       "      <td>-0.212660</td>\n",
       "      <td>1.049921</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.106984</td>\n",
       "      <td>0.729727</td>\n",
       "      <td>-0.161666</td>\n",
       "      <td>0.312561</td>\n",
       "      <td>-0.414116</td>\n",
       "      <td>1.071126</td>\n",
       "      <td>0.023712</td>\n",
       "      <td>0.419117</td>\n",
       "      <td>14278.97</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0 -0.260648 -0.469648  2.496266 -0.083724  0.129681  0.732898  0.519014   \n",
       "1   1  0.985100 -0.356045  0.558056 -0.429654  0.277140  0.428605  0.406466   \n",
       "2   2 -0.260272 -0.949385  1.728538 -0.457986  0.074062  1.419481  0.743511   \n",
       "3   3 -0.152152 -0.508959  1.746840 -1.090178  0.249486  1.143312  0.518269   \n",
       "4   4 -0.206820 -0.165280  1.527053 -0.448293  0.106125  0.530549  0.658849   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0 -0.130006  0.727159  ... -0.110552  0.217606 -0.134794  0.165959  0.126280   \n",
       "1 -0.133118  0.347452  ... -0.194936 -0.605761  0.079469 -0.577395  0.190090   \n",
       "2 -0.095576 -0.261297  ... -0.005020  0.702906  0.945045 -1.154666 -0.605564   \n",
       "3 -0.065130 -0.205698  ... -0.146927 -0.038212 -0.214048 -1.893131  1.003963   \n",
       "4 -0.212660  1.049921  ... -0.106984  0.729727 -0.161666  0.312561 -0.414116   \n",
       "\n",
       "        V26       V27       V28    Amount  Class  \n",
       "0 -0.434824 -0.081230 -0.151045  17982.10      0  \n",
       "1  0.296503 -0.248052 -0.064512   6531.37      0  \n",
       "2 -0.312895 -0.300258 -0.244718   2513.54      0  \n",
       "3 -0.515950 -0.165316  0.048424   5384.44      0  \n",
       "4  1.071126  0.023712  0.419117  14278.97      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1a6a875-e5b8-40e8-acf3-81588eb7681b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data : (568630, 31)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of data : {data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609a9426-0171-4302-ba09-e3e5b22326e2",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474b8256-2bf6-4d80-84e7-2373b9e69181",
   "metadata": {},
   "source": [
    "##### Checking for duplicated data points and removing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e19273e9-9ad7-4493-a5d0-d6161a9ef1fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>-0.260648</th>\n",
       "      <th>-0.469648</th>\n",
       "      <th>2.496266</th>\n",
       "      <th>-0.083724</th>\n",
       "      <th>0.129681</th>\n",
       "      <th>0.732898</th>\n",
       "      <th>0.519014</th>\n",
       "      <th>-0.130006</th>\n",
       "      <th>0.727159</th>\n",
       "      <th>0.637735</th>\n",
       "      <th>-0.987020</th>\n",
       "      <th>0.293438</th>\n",
       "      <th>-0.941386</th>\n",
       "      <th>0.549020</th>\n",
       "      <th>1.804879</th>\n",
       "      <th>0.215598</th>\n",
       "      <th>0.512307</th>\n",
       "      <th>0.333644</th>\n",
       "      <th>0.124270</th>\n",
       "      <th>0.091202</th>\n",
       "      <th>-0.110552</th>\n",
       "      <th>0.217606</th>\n",
       "      <th>-0.134794</th>\n",
       "      <th>0.165959</th>\n",
       "      <th>0.126280</th>\n",
       "      <th>-0.434824</th>\n",
       "      <th>-0.081230</th>\n",
       "      <th>-0.151045</th>\n",
       "      <th>17982.10</th>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379089</th>\n",
       "      <th>0.211525</th>\n",
       "      <th>-0.172433</th>\n",
       "      <th>0.389090</th>\n",
       "      <th>-0.520573</th>\n",
       "      <th>0.605021</th>\n",
       "      <th>0.154907</th>\n",
       "      <th>0.629336</th>\n",
       "      <th>-0.149879</th>\n",
       "      <th>0.376518</th>\n",
       "      <th>0.485291</th>\n",
       "      <th>-0.062064</th>\n",
       "      <th>0.695621</th>\n",
       "      <th>-0.620391</th>\n",
       "      <th>0.341974</th>\n",
       "      <th>0.055285</th>\n",
       "      <th>0.501684</th>\n",
       "      <th>0.734376</th>\n",
       "      <th>0.561183</th>\n",
       "      <th>0.332665</th>\n",
       "      <th>-0.122305</th>\n",
       "      <th>-0.190367</th>\n",
       "      <th>-0.483677</th>\n",
       "      <th>0.040308</th>\n",
       "      <th>1.227441</th>\n",
       "      <th>-0.864953</th>\n",
       "      <th>0.369719</th>\n",
       "      <th>-0.180891</th>\n",
       "      <th>-0.130064</th>\n",
       "      <th>5114.32</th>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379083</th>\n",
       "      <th>-0.056224</th>\n",
       "      <th>0.812127</th>\n",
       "      <th>-0.901054</th>\n",
       "      <th>1.340335</th>\n",
       "      <th>0.572430</th>\n",
       "      <th>-1.108707</th>\n",
       "      <th>-0.168552</th>\n",
       "      <th>0.044594</th>\n",
       "      <th>-1.323992</th>\n",
       "      <th>-0.901344</th>\n",
       "      <th>0.927204</th>\n",
       "      <th>-1.148801</th>\n",
       "      <th>-0.729588</th>\n",
       "      <th>-1.585337</th>\n",
       "      <th>0.657354</th>\n",
       "      <th>-0.459101</th>\n",
       "      <th>-0.253884</th>\n",
       "      <th>0.267426</th>\n",
       "      <th>-0.384456</th>\n",
       "      <th>0.549243</th>\n",
       "      <th>0.106539</th>\n",
       "      <th>-0.598813</th>\n",
       "      <th>-0.222497</th>\n",
       "      <th>-0.920982</th>\n",
       "      <th>-0.234792</th>\n",
       "      <th>0.728264</th>\n",
       "      <th>0.856948</th>\n",
       "      <th>0.942765</th>\n",
       "      <th>21612.50</th>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379084</th>\n",
       "      <th>-0.661333</th>\n",
       "      <th>0.718460</th>\n",
       "      <th>-0.627551</th>\n",
       "      <th>1.009158</th>\n",
       "      <th>-0.610775</th>\n",
       "      <th>0.162125</th>\n",
       "      <th>-0.632635</th>\n",
       "      <th>0.014514</th>\n",
       "      <th>-1.030040</th>\n",
       "      <th>-0.755540</th>\n",
       "      <th>0.853292</th>\n",
       "      <th>-1.100166</th>\n",
       "      <th>-0.388581</th>\n",
       "      <th>-1.093695</th>\n",
       "      <th>0.672853</th>\n",
       "      <th>-1.310834</th>\n",
       "      <th>-1.149475</th>\n",
       "      <th>-1.409933</th>\n",
       "      <th>0.803254</th>\n",
       "      <th>0.051403</th>\n",
       "      <th>0.673335</th>\n",
       "      <th>-0.309568</th>\n",
       "      <th>0.318867</th>\n",
       "      <th>-0.280152</th>\n",
       "      <th>-0.691953</th>\n",
       "      <th>0.310704</th>\n",
       "      <th>0.617553</th>\n",
       "      <th>0.538272</th>\n",
       "      <th>3875.05</th>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379085</th>\n",
       "      <th>0.908772</th>\n",
       "      <th>-0.120471</th>\n",
       "      <th>0.259350</th>\n",
       "      <th>0.334283</th>\n",
       "      <th>0.530768</th>\n",
       "      <th>0.306387</th>\n",
       "      <th>0.485395</th>\n",
       "      <th>-0.116964</th>\n",
       "      <th>-0.046796</th>\n",
       "      <th>0.492581</th>\n",
       "      <th>0.286629</th>\n",
       "      <th>0.413548</th>\n",
       "      <th>-1.123131</th>\n",
       "      <th>-0.019479</th>\n",
       "      <th>0.359567</th>\n",
       "      <th>1.681941</th>\n",
       "      <th>1.137343</th>\n",
       "      <th>1.205748</th>\n",
       "      <th>-1.577292</th>\n",
       "      <th>-0.327593</th>\n",
       "      <th>-0.131594</th>\n",
       "      <th>-0.258222</th>\n",
       "      <th>-0.100895</th>\n",
       "      <th>-0.415285</th>\n",
       "      <th>0.660461</th>\n",
       "      <th>0.153713</th>\n",
       "      <th>-0.212130</th>\n",
       "      <th>0.053075</th>\n",
       "      <th>13369.44</th>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379086</th>\n",
       "      <th>-0.988588</th>\n",
       "      <th>0.210995</th>\n",
       "      <th>-0.685727</th>\n",
       "      <th>0.832897</th>\n",
       "      <th>-0.717356</th>\n",
       "      <th>0.143710</th>\n",
       "      <th>-0.654842</th>\n",
       "      <th>-0.027151</th>\n",
       "      <th>-0.705800</th>\n",
       "      <th>-0.723645</th>\n",
       "      <th>0.816733</th>\n",
       "      <th>-0.932878</th>\n",
       "      <th>-0.331674</th>\n",
       "      <th>-0.936884</th>\n",
       "      <th>0.922593</th>\n",
       "      <th>-0.937358</th>\n",
       "      <th>-1.074161</th>\n",
       "      <th>-0.904533</th>\n",
       "      <th>0.678118</th>\n",
       "      <th>-0.760861</th>\n",
       "      <th>-0.049257</th>\n",
       "      <th>0.656853</th>\n",
       "      <th>0.249006</th>\n",
       "      <th>-0.274331</th>\n",
       "      <th>-0.294853</th>\n",
       "      <th>-0.389519</th>\n",
       "      <th>-0.643844</th>\n",
       "      <th>0.237237</th>\n",
       "      <th>23063.22</th>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379087</th>\n",
       "      <th>0.060282</th>\n",
       "      <th>0.576045</th>\n",
       "      <th>-0.739466</th>\n",
       "      <th>1.076765</th>\n",
       "      <th>1.078398</th>\n",
       "      <th>-0.947936</th>\n",
       "      <th>0.163517</th>\n",
       "      <th>-0.001971</th>\n",
       "      <th>-1.080344</th>\n",
       "      <th>-0.530279</th>\n",
       "      <th>1.011454</th>\n",
       "      <th>-0.681080</th>\n",
       "      <th>-0.825464</th>\n",
       "      <th>-1.279849</th>\n",
       "      <th>-0.577576</th>\n",
       "      <th>0.664725</th>\n",
       "      <th>0.632843</th>\n",
       "      <th>1.908391</th>\n",
       "      <th>-1.322008</th>\n",
       "      <th>0.190153</th>\n",
       "      <th>0.011185</th>\n",
       "      <th>-0.507180</th>\n",
       "      <th>-0.099939</th>\n",
       "      <th>-0.959833</th>\n",
       "      <th>-0.847559</th>\n",
       "      <th>0.172763</th>\n",
       "      <th>0.372449</th>\n",
       "      <th>0.617530</th>\n",
       "      <th>3659.93</th>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379088</th>\n",
       "      <th>-0.920784</th>\n",
       "      <th>0.253166</th>\n",
       "      <th>-0.986008</th>\n",
       "      <th>0.576595</th>\n",
       "      <th>-0.612349</th>\n",
       "      <th>-1.121961</th>\n",
       "      <th>-0.514484</th>\n",
       "      <th>0.447629</th>\n",
       "      <th>-0.940910</th>\n",
       "      <th>-1.038442</th>\n",
       "      <th>0.958835</th>\n",
       "      <th>-1.041447</th>\n",
       "      <th>-0.325763</th>\n",
       "      <th>-1.195772</th>\n",
       "      <th>-0.176406</th>\n",
       "      <th>-0.837172</th>\n",
       "      <th>-1.001923</th>\n",
       "      <th>-0.819470</th>\n",
       "      <th>0.171095</th>\n",
       "      <th>0.055188</th>\n",
       "      <th>0.198699</th>\n",
       "      <th>-0.370894</th>\n",
       "      <th>-0.169600</th>\n",
       "      <th>-0.450099</th>\n",
       "      <th>0.008034</th>\n",
       "      <th>1.729979</th>\n",
       "      <th>1.571240</th>\n",
       "      <th>-0.898110</th>\n",
       "      <th>507.83</th>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379090</th>\n",
       "      <th>-1.864083</th>\n",
       "      <th>1.771047</th>\n",
       "      <th>-1.801018</th>\n",
       "      <th>1.555376</th>\n",
       "      <th>-2.225492</th>\n",
       "      <th>-1.801494</th>\n",
       "      <th>-1.947294</th>\n",
       "      <th>2.786778</th>\n",
       "      <th>-1.685096</th>\n",
       "      <th>-1.692345</th>\n",
       "      <th>1.380750</th>\n",
       "      <th>-1.490524</th>\n",
       "      <th>1.822770</th>\n",
       "      <th>-1.128814</th>\n",
       "      <th>-0.079670</th>\n",
       "      <th>-1.666332</th>\n",
       "      <th>-1.889376</th>\n",
       "      <th>-1.874638</th>\n",
       "      <th>1.559518</th>\n",
       "      <th>-0.356620</th>\n",
       "      <th>0.783244</th>\n",
       "      <th>-0.419915</th>\n",
       "      <th>-0.882868</th>\n",
       "      <th>0.933353</th>\n",
       "      <th>0.522755</th>\n",
       "      <th>-0.105125</th>\n",
       "      <th>-0.764064</th>\n",
       "      <th>-1.024260</th>\n",
       "      <th>15846.81</th>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379098</th>\n",
       "      <th>0.366211</th>\n",
       "      <th>0.555796</th>\n",
       "      <th>-0.764675</th>\n",
       "      <th>1.121016</th>\n",
       "      <th>1.010087</th>\n",
       "      <th>-0.950854</th>\n",
       "      <th>0.185136</th>\n",
       "      <th>-0.071981</th>\n",
       "      <th>-1.106592</th>\n",
       "      <th>-0.605405</th>\n",
       "      <th>1.245389</th>\n",
       "      <th>-0.857179</th>\n",
       "      <th>-2.556022</th>\n",
       "      <th>-1.329595</th>\n",
       "      <th>-0.302395</th>\n",
       "      <th>0.135132</th>\n",
       "      <th>0.583234</th>\n",
       "      <th>1.096294</th>\n",
       "      <th>-2.051636</th>\n",
       "      <th>0.151008</th>\n",
       "      <th>0.038532</th>\n",
       "      <th>-0.351939</th>\n",
       "      <th>-0.541486</th>\n",
       "      <th>-0.844571</th>\n",
       "      <th>2.217356</th>\n",
       "      <th>1.147506</th>\n",
       "      <th>0.456721</th>\n",
       "      <th>0.860927</th>\n",
       "      <th>22966.06</th>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                               count\n",
       "id     V1        V2        V3        V4        V5        V6        V7        V8        V9        V10       V11       V12       V13       V14       V15       V16       V17       V18       V19       V20       V21       V22       V23       V24       V25       V26       V27       V28       Amount   Class       \n",
       "0      -0.260648 -0.469648  2.496266 -0.083724  0.129681  0.732898  0.519014 -0.130006  0.727159  0.637735 -0.987020  0.293438 -0.941386  0.549020  1.804879  0.215598  0.512307  0.333644  0.124270  0.091202 -0.110552  0.217606 -0.134794  0.165959  0.126280 -0.434824 -0.081230 -0.151045 17982.10 0          1\n",
       "379089  0.211525 -0.172433  0.389090 -0.520573  0.605021  0.154907  0.629336 -0.149879  0.376518  0.485291 -0.062064  0.695621 -0.620391  0.341974  0.055285  0.501684  0.734376  0.561183  0.332665 -0.122305 -0.190367 -0.483677  0.040308  1.227441 -0.864953  0.369719 -0.180891 -0.130064 5114.32  1          1\n",
       "379083 -0.056224  0.812127 -0.901054  1.340335  0.572430 -1.108707 -0.168552  0.044594 -1.323992 -0.901344  0.927204 -1.148801 -0.729588 -1.585337  0.657354 -0.459101 -0.253884  0.267426 -0.384456  0.549243  0.106539 -0.598813 -0.222497 -0.920982 -0.234792  0.728264  0.856948  0.942765 21612.50 1          1\n",
       "379084 -0.661333  0.718460 -0.627551  1.009158 -0.610775  0.162125 -0.632635  0.014514 -1.030040 -0.755540  0.853292 -1.100166 -0.388581 -1.093695  0.672853 -1.310834 -1.149475 -1.409933  0.803254  0.051403  0.673335 -0.309568  0.318867 -0.280152 -0.691953  0.310704  0.617553  0.538272 3875.05  1          1\n",
       "379085  0.908772 -0.120471  0.259350  0.334283  0.530768  0.306387  0.485395 -0.116964 -0.046796  0.492581  0.286629  0.413548 -1.123131 -0.019479  0.359567  1.681941  1.137343  1.205748 -1.577292 -0.327593 -0.131594 -0.258222 -0.100895 -0.415285  0.660461  0.153713 -0.212130  0.053075 13369.44 1          1\n",
       "379086 -0.988588  0.210995 -0.685727  0.832897 -0.717356  0.143710 -0.654842 -0.027151 -0.705800 -0.723645  0.816733 -0.932878 -0.331674 -0.936884  0.922593 -0.937358 -1.074161 -0.904533  0.678118 -0.760861 -0.049257  0.656853  0.249006 -0.274331 -0.294853 -0.389519 -0.643844  0.237237 23063.22 1          1\n",
       "379087  0.060282  0.576045 -0.739466  1.076765  1.078398 -0.947936  0.163517 -0.001971 -1.080344 -0.530279  1.011454 -0.681080 -0.825464 -1.279849 -0.577576  0.664725  0.632843  1.908391 -1.322008  0.190153  0.011185 -0.507180 -0.099939 -0.959833 -0.847559  0.172763  0.372449  0.617530 3659.93  1          1\n",
       "379088 -0.920784  0.253166 -0.986008  0.576595 -0.612349 -1.121961 -0.514484  0.447629 -0.940910 -1.038442  0.958835 -1.041447 -0.325763 -1.195772 -0.176406 -0.837172 -1.001923 -0.819470  0.171095  0.055188  0.198699 -0.370894 -0.169600 -0.450099  0.008034  1.729979  1.571240 -0.898110 507.83   1          1\n",
       "379090 -1.864083  1.771047 -1.801018  1.555376 -2.225492 -1.801494 -1.947294  2.786778 -1.685096 -1.692345  1.380750 -1.490524  1.822770 -1.128814 -0.079670 -1.666332 -1.889376 -1.874638  1.559518 -0.356620  0.783244 -0.419915 -0.882868  0.933353  0.522755 -0.105125 -0.764064 -1.024260 15846.81 1          1\n",
       "379098  0.366211  0.555796 -0.764675  1.121016  1.010087 -0.950854  0.185136 -0.071981 -1.106592 -0.605405  1.245389 -0.857179 -2.556022 -1.329595 -0.302395  0.135132  0.583234  1.096294 -2.051636  0.151008  0.038532 -0.351939 -0.541486 -0.844571  2.217356  1.147506  0.456721  0.860927 22966.06 1          1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicated_values = data.value_counts()\n",
    "duplicated_values = duplicated_values.to_frame()\n",
    "duplicated_values.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b17d6af-dd68-4c41-a5e4-babb6a726dcc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.260648</td>\n",
       "      <td>-0.469648</td>\n",
       "      <td>2.496266</td>\n",
       "      <td>-0.083724</td>\n",
       "      <td>0.129681</td>\n",
       "      <td>0.732898</td>\n",
       "      <td>0.519014</td>\n",
       "      <td>-0.130006</td>\n",
       "      <td>0.727159</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.110552</td>\n",
       "      <td>0.217606</td>\n",
       "      <td>-0.134794</td>\n",
       "      <td>0.165959</td>\n",
       "      <td>0.126280</td>\n",
       "      <td>-0.434824</td>\n",
       "      <td>-0.081230</td>\n",
       "      <td>-0.151045</td>\n",
       "      <td>17982.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.985100</td>\n",
       "      <td>-0.356045</td>\n",
       "      <td>0.558056</td>\n",
       "      <td>-0.429654</td>\n",
       "      <td>0.277140</td>\n",
       "      <td>0.428605</td>\n",
       "      <td>0.406466</td>\n",
       "      <td>-0.133118</td>\n",
       "      <td>0.347452</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.194936</td>\n",
       "      <td>-0.605761</td>\n",
       "      <td>0.079469</td>\n",
       "      <td>-0.577395</td>\n",
       "      <td>0.190090</td>\n",
       "      <td>0.296503</td>\n",
       "      <td>-0.248052</td>\n",
       "      <td>-0.064512</td>\n",
       "      <td>6531.37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.260272</td>\n",
       "      <td>-0.949385</td>\n",
       "      <td>1.728538</td>\n",
       "      <td>-0.457986</td>\n",
       "      <td>0.074062</td>\n",
       "      <td>1.419481</td>\n",
       "      <td>0.743511</td>\n",
       "      <td>-0.095576</td>\n",
       "      <td>-0.261297</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005020</td>\n",
       "      <td>0.702906</td>\n",
       "      <td>0.945045</td>\n",
       "      <td>-1.154666</td>\n",
       "      <td>-0.605564</td>\n",
       "      <td>-0.312895</td>\n",
       "      <td>-0.300258</td>\n",
       "      <td>-0.244718</td>\n",
       "      <td>2513.54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.152152</td>\n",
       "      <td>-0.508959</td>\n",
       "      <td>1.746840</td>\n",
       "      <td>-1.090178</td>\n",
       "      <td>0.249486</td>\n",
       "      <td>1.143312</td>\n",
       "      <td>0.518269</td>\n",
       "      <td>-0.065130</td>\n",
       "      <td>-0.205698</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.146927</td>\n",
       "      <td>-0.038212</td>\n",
       "      <td>-0.214048</td>\n",
       "      <td>-1.893131</td>\n",
       "      <td>1.003963</td>\n",
       "      <td>-0.515950</td>\n",
       "      <td>-0.165316</td>\n",
       "      <td>0.048424</td>\n",
       "      <td>5384.44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.206820</td>\n",
       "      <td>-0.165280</td>\n",
       "      <td>1.527053</td>\n",
       "      <td>-0.448293</td>\n",
       "      <td>0.106125</td>\n",
       "      <td>0.530549</td>\n",
       "      <td>0.658849</td>\n",
       "      <td>-0.212660</td>\n",
       "      <td>1.049921</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.106984</td>\n",
       "      <td>0.729727</td>\n",
       "      <td>-0.161666</td>\n",
       "      <td>0.312561</td>\n",
       "      <td>-0.414116</td>\n",
       "      <td>1.071126</td>\n",
       "      <td>0.023712</td>\n",
       "      <td>0.419117</td>\n",
       "      <td>14278.97</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0 -0.260648 -0.469648  2.496266 -0.083724  0.129681  0.732898  0.519014   \n",
       "1   1  0.985100 -0.356045  0.558056 -0.429654  0.277140  0.428605  0.406466   \n",
       "2   2 -0.260272 -0.949385  1.728538 -0.457986  0.074062  1.419481  0.743511   \n",
       "3   3 -0.152152 -0.508959  1.746840 -1.090178  0.249486  1.143312  0.518269   \n",
       "4   4 -0.206820 -0.165280  1.527053 -0.448293  0.106125  0.530549  0.658849   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0 -0.130006  0.727159  ... -0.110552  0.217606 -0.134794  0.165959  0.126280   \n",
       "1 -0.133118  0.347452  ... -0.194936 -0.605761  0.079469 -0.577395  0.190090   \n",
       "2 -0.095576 -0.261297  ... -0.005020  0.702906  0.945045 -1.154666 -0.605564   \n",
       "3 -0.065130 -0.205698  ... -0.146927 -0.038212 -0.214048 -1.893131  1.003963   \n",
       "4 -0.212660  1.049921  ... -0.106984  0.729727 -0.161666  0.312561 -0.414116   \n",
       "\n",
       "        V26       V27       V28    Amount  Class  \n",
       "0 -0.434824 -0.081230 -0.151045  17982.10      0  \n",
       "1  0.296503 -0.248052 -0.064512   6531.37      0  \n",
       "2 -0.312895 -0.300258 -0.244718   2513.54      0  \n",
       "3 -0.515950 -0.165316  0.048424   5384.44      0  \n",
       "4  1.071126  0.023712  0.419117  14278.97      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop_duplicates()\n",
    "data = data.reset_index()\n",
    "data = data.drop(['index'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1fbd3e7-c90c-4837-ba6c-b2e47c7a3f79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of updated data : (568630, 31)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of updated data : {data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095a0b9a-6a01-4dbf-a534-e83b229a3ac9",
   "metadata": {},
   "source": [
    "##### Checking NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ac256b0-cec7-41e8-be4e-c749ce6eceef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id        0\n",
       "V1        0\n",
       "V2        0\n",
       "V3        0\n",
       "V4        0\n",
       "V5        0\n",
       "V6        0\n",
       "V7        0\n",
       "V8        0\n",
       "V9        0\n",
       "V10       0\n",
       "V11       0\n",
       "V12       0\n",
       "V13       0\n",
       "V14       0\n",
       "V15       0\n",
       "V16       0\n",
       "V17       0\n",
       "V18       0\n",
       "V19       0\n",
       "V20       0\n",
       "V21       0\n",
       "V22       0\n",
       "V23       0\n",
       "V24       0\n",
       "V25       0\n",
       "V26       0\n",
       "V27       0\n",
       "V28       0\n",
       "Amount    0\n",
       "Class     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6784401f-3a6b-4582-ba24-37cc3d548685",
   "metadata": {},
   "source": [
    "#### Checking the distribution of the Class and Undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6427c5-f92f-4f52-870c-15ef925ee91e",
   "metadata": {},
   "source": [
    "Here, output y is called class\n",
    "* If Class = 1 -> Fraud Transaction\n",
    "* If Class = 0 -> Legit Transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fff9a36-fb3d-4699-a2d8-c65383a0749f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "0    284315\n",
       "1    284315\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d102fa23-7e05-464b-85ca-527c540d549c",
   "metadata": {},
   "source": [
    "The data exhibits a significant bias towards legitimate transactions. A basic model predicting legitimate transactions every time would yield an accuracy of over 99%, which is not ideal. Therefore, #undersampling the data is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6374fc24-1eaa-4866-a85c-5a8b5a684ad5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "legit_data = data[data['Class'] == 0]\n",
    "fraud_data = data[data['Class'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2f2d7dc-5165-4cd7-83f4-860029b254fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Legit Data : (284315, 31)\n",
      "Shape of Fraud Data : (284315, 31)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of Legit Data : {legit_data.shape}\")\n",
    "print(f\"Shape of Fraud Data : {fraud_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ced398-a115-4616-b5ff-63917b6d1581",
   "metadata": {},
   "source": [
    "Taking random samples from Legit Data same as size of Fraud Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e585890-d1f2-4b53-9124-e794f685f300",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "legit_data_updated = legit_data.sample(n = fraud_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe406513-2c26-4766-9e2b-fc8f6b11326c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Legit Data : (284315, 31)\n",
      "Shape of Fraud Data : (284315, 31)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Shape of Legit Data : {legit_data_updated.shape}\")\n",
    "print(f\"Shape of Fraud Data : {fraud_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2502d8ba-fd87-4bd3-9bec-d9209bcf63fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55001</th>\n",
       "      <td>55001</td>\n",
       "      <td>-0.157158</td>\n",
       "      <td>-0.624370</td>\n",
       "      <td>1.904319</td>\n",
       "      <td>0.002548</td>\n",
       "      <td>-0.063530</td>\n",
       "      <td>1.569317</td>\n",
       "      <td>0.060122</td>\n",
       "      <td>-0.379401</td>\n",
       "      <td>1.219746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.383390</td>\n",
       "      <td>-1.018242</td>\n",
       "      <td>-2.242913</td>\n",
       "      <td>-0.238905</td>\n",
       "      <td>1.015113</td>\n",
       "      <td>-0.398106</td>\n",
       "      <td>0.421939</td>\n",
       "      <td>0.227771</td>\n",
       "      <td>9232.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218980</th>\n",
       "      <td>218980</td>\n",
       "      <td>-0.561426</td>\n",
       "      <td>-1.859024</td>\n",
       "      <td>0.952442</td>\n",
       "      <td>-1.917161</td>\n",
       "      <td>0.186360</td>\n",
       "      <td>-0.261528</td>\n",
       "      <td>-0.138172</td>\n",
       "      <td>-0.710571</td>\n",
       "      <td>1.689376</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.299712</td>\n",
       "      <td>-0.162900</td>\n",
       "      <td>-3.940340</td>\n",
       "      <td>0.280969</td>\n",
       "      <td>0.599533</td>\n",
       "      <td>-0.818793</td>\n",
       "      <td>-0.264296</td>\n",
       "      <td>0.953107</td>\n",
       "      <td>12767.83</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183068</th>\n",
       "      <td>183068</td>\n",
       "      <td>1.733065</td>\n",
       "      <td>-0.484927</td>\n",
       "      <td>-0.338547</td>\n",
       "      <td>-0.508672</td>\n",
       "      <td>1.844365</td>\n",
       "      <td>2.149494</td>\n",
       "      <td>0.423823</td>\n",
       "      <td>0.009915</td>\n",
       "      <td>0.542049</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.097469</td>\n",
       "      <td>0.060500</td>\n",
       "      <td>0.030920</td>\n",
       "      <td>1.513812</td>\n",
       "      <td>0.658086</td>\n",
       "      <td>-1.289520</td>\n",
       "      <td>-0.234373</td>\n",
       "      <td>-0.254534</td>\n",
       "      <td>9319.29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83369</th>\n",
       "      <td>83369</td>\n",
       "      <td>0.161355</td>\n",
       "      <td>-0.722348</td>\n",
       "      <td>1.919677</td>\n",
       "      <td>-1.596573</td>\n",
       "      <td>-0.233721</td>\n",
       "      <td>0.402011</td>\n",
       "      <td>0.318655</td>\n",
       "      <td>-0.175508</td>\n",
       "      <td>-0.564956</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.068540</td>\n",
       "      <td>0.547724</td>\n",
       "      <td>0.028902</td>\n",
       "      <td>0.633042</td>\n",
       "      <td>-0.876625</td>\n",
       "      <td>-0.279478</td>\n",
       "      <td>-0.251008</td>\n",
       "      <td>-0.289524</td>\n",
       "      <td>16565.23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113630</th>\n",
       "      <td>113630</td>\n",
       "      <td>-0.031355</td>\n",
       "      <td>-0.219666</td>\n",
       "      <td>1.487046</td>\n",
       "      <td>-0.116659</td>\n",
       "      <td>0.280620</td>\n",
       "      <td>0.567073</td>\n",
       "      <td>0.558993</td>\n",
       "      <td>-0.070880</td>\n",
       "      <td>0.330330</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.092115</td>\n",
       "      <td>0.201758</td>\n",
       "      <td>-0.085954</td>\n",
       "      <td>0.423868</td>\n",
       "      <td>-0.401917</td>\n",
       "      <td>-0.580871</td>\n",
       "      <td>0.261089</td>\n",
       "      <td>0.372878</td>\n",
       "      <td>22246.09</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568625</th>\n",
       "      <td>568625</td>\n",
       "      <td>-0.833437</td>\n",
       "      <td>0.061886</td>\n",
       "      <td>-0.899794</td>\n",
       "      <td>0.904227</td>\n",
       "      <td>-1.002401</td>\n",
       "      <td>0.481454</td>\n",
       "      <td>-0.370393</td>\n",
       "      <td>0.189694</td>\n",
       "      <td>-0.938153</td>\n",
       "      <td>...</td>\n",
       "      <td>0.167503</td>\n",
       "      <td>0.419731</td>\n",
       "      <td>1.288249</td>\n",
       "      <td>-0.900861</td>\n",
       "      <td>0.560661</td>\n",
       "      <td>-0.006018</td>\n",
       "      <td>3.308968</td>\n",
       "      <td>0.081564</td>\n",
       "      <td>4394.16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568626</th>\n",
       "      <td>568626</td>\n",
       "      <td>-0.670459</td>\n",
       "      <td>-0.202896</td>\n",
       "      <td>-0.068129</td>\n",
       "      <td>-0.267328</td>\n",
       "      <td>-0.133660</td>\n",
       "      <td>0.237148</td>\n",
       "      <td>-0.016935</td>\n",
       "      <td>-0.147733</td>\n",
       "      <td>0.483894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031874</td>\n",
       "      <td>0.388161</td>\n",
       "      <td>-0.154257</td>\n",
       "      <td>-0.846452</td>\n",
       "      <td>-0.153443</td>\n",
       "      <td>1.961398</td>\n",
       "      <td>-1.528642</td>\n",
       "      <td>1.704306</td>\n",
       "      <td>4653.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568627</th>\n",
       "      <td>568627</td>\n",
       "      <td>-0.311997</td>\n",
       "      <td>-0.004095</td>\n",
       "      <td>0.137526</td>\n",
       "      <td>-0.035893</td>\n",
       "      <td>-0.042291</td>\n",
       "      <td>0.121098</td>\n",
       "      <td>-0.070958</td>\n",
       "      <td>-0.019997</td>\n",
       "      <td>-0.122048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140788</td>\n",
       "      <td>0.536523</td>\n",
       "      <td>-0.211100</td>\n",
       "      <td>-0.448909</td>\n",
       "      <td>0.540073</td>\n",
       "      <td>-0.755836</td>\n",
       "      <td>-0.487540</td>\n",
       "      <td>-0.268741</td>\n",
       "      <td>23572.85</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568628</th>\n",
       "      <td>568628</td>\n",
       "      <td>0.636871</td>\n",
       "      <td>-0.516970</td>\n",
       "      <td>-0.300889</td>\n",
       "      <td>-0.144480</td>\n",
       "      <td>0.131042</td>\n",
       "      <td>-0.294148</td>\n",
       "      <td>0.580568</td>\n",
       "      <td>-0.207723</td>\n",
       "      <td>0.893527</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060381</td>\n",
       "      <td>-0.195609</td>\n",
       "      <td>-0.175488</td>\n",
       "      <td>-0.554643</td>\n",
       "      <td>-0.099669</td>\n",
       "      <td>-1.434931</td>\n",
       "      <td>-0.159269</td>\n",
       "      <td>-0.076251</td>\n",
       "      <td>10160.83</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568629</th>\n",
       "      <td>568629</td>\n",
       "      <td>-0.795144</td>\n",
       "      <td>0.433236</td>\n",
       "      <td>-0.649140</td>\n",
       "      <td>0.374732</td>\n",
       "      <td>-0.244976</td>\n",
       "      <td>-0.603493</td>\n",
       "      <td>-0.347613</td>\n",
       "      <td>-0.340814</td>\n",
       "      <td>0.253971</td>\n",
       "      <td>...</td>\n",
       "      <td>0.534853</td>\n",
       "      <td>-0.291514</td>\n",
       "      <td>0.157303</td>\n",
       "      <td>0.931030</td>\n",
       "      <td>-0.349423</td>\n",
       "      <td>-1.090974</td>\n",
       "      <td>-1.575113</td>\n",
       "      <td>0.722936</td>\n",
       "      <td>21493.92</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>568630 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id        V1        V2        V3        V4        V5        V6  \\\n",
       "55001    55001 -0.157158 -0.624370  1.904319  0.002548 -0.063530  1.569317   \n",
       "218980  218980 -0.561426 -1.859024  0.952442 -1.917161  0.186360 -0.261528   \n",
       "183068  183068  1.733065 -0.484927 -0.338547 -0.508672  1.844365  2.149494   \n",
       "83369    83369  0.161355 -0.722348  1.919677 -1.596573 -0.233721  0.402011   \n",
       "113630  113630 -0.031355 -0.219666  1.487046 -0.116659  0.280620  0.567073   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "568625  568625 -0.833437  0.061886 -0.899794  0.904227 -1.002401  0.481454   \n",
       "568626  568626 -0.670459 -0.202896 -0.068129 -0.267328 -0.133660  0.237148   \n",
       "568627  568627 -0.311997 -0.004095  0.137526 -0.035893 -0.042291  0.121098   \n",
       "568628  568628  0.636871 -0.516970 -0.300889 -0.144480  0.131042 -0.294148   \n",
       "568629  568629 -0.795144  0.433236 -0.649140  0.374732 -0.244976 -0.603493   \n",
       "\n",
       "              V7        V8        V9  ...       V21       V22       V23  \\\n",
       "55001   0.060122 -0.379401  1.219746  ...  0.383390 -1.018242 -2.242913   \n",
       "218980 -0.138172 -0.710571  1.689376  ... -0.299712 -0.162900 -3.940340   \n",
       "183068  0.423823  0.009915  0.542049  ... -0.097469  0.060500  0.030920   \n",
       "83369   0.318655 -0.175508 -0.564956  ... -0.068540  0.547724  0.028902   \n",
       "113630  0.558993 -0.070880  0.330330  ... -0.092115  0.201758 -0.085954   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "568625 -0.370393  0.189694 -0.938153  ...  0.167503  0.419731  1.288249   \n",
       "568626 -0.016935 -0.147733  0.483894  ...  0.031874  0.388161 -0.154257   \n",
       "568627 -0.070958 -0.019997 -0.122048  ...  0.140788  0.536523 -0.211100   \n",
       "568628  0.580568 -0.207723  0.893527  ... -0.060381 -0.195609 -0.175488   \n",
       "568629 -0.347613 -0.340814  0.253971  ...  0.534853 -0.291514  0.157303   \n",
       "\n",
       "             V24       V25       V26       V27       V28    Amount  Class  \n",
       "55001  -0.238905  1.015113 -0.398106  0.421939  0.227771   9232.58      0  \n",
       "218980  0.280969  0.599533 -0.818793 -0.264296  0.953107  12767.83      0  \n",
       "183068  1.513812  0.658086 -1.289520 -0.234373 -0.254534   9319.29      0  \n",
       "83369   0.633042 -0.876625 -0.279478 -0.251008 -0.289524  16565.23      0  \n",
       "113630  0.423868 -0.401917 -0.580871  0.261089  0.372878  22246.09      0  \n",
       "...          ...       ...       ...       ...       ...       ...    ...  \n",
       "568625 -0.900861  0.560661 -0.006018  3.308968  0.081564   4394.16      1  \n",
       "568626 -0.846452 -0.153443  1.961398 -1.528642  1.704306   4653.40      1  \n",
       "568627 -0.448909  0.540073 -0.755836 -0.487540 -0.268741  23572.85      1  \n",
       "568628 -0.554643 -0.099669 -1.434931 -0.159269 -0.076251  10160.83      1  \n",
       "568629  0.931030 -0.349423 -1.090974 -1.575113  0.722936  21493.92      1  \n",
       "\n",
       "[568630 rows x 31 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the data\n",
    "data_updated = pd.concat([legit_data_updated, fraud_data], axis = 0)\n",
    "data_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a752921-fd1f-41b6-b499-b42a6849df75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>529137</th>\n",
       "      <td>529137</td>\n",
       "      <td>-0.928640</td>\n",
       "      <td>-0.238530</td>\n",
       "      <td>-0.757097</td>\n",
       "      <td>0.020714</td>\n",
       "      <td>0.117750</td>\n",
       "      <td>0.200009</td>\n",
       "      <td>-0.525105</td>\n",
       "      <td>-0.294222</td>\n",
       "      <td>-0.816412</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.226424</td>\n",
       "      <td>1.428466</td>\n",
       "      <td>-0.231378</td>\n",
       "      <td>-0.429209</td>\n",
       "      <td>1.398836</td>\n",
       "      <td>1.700747</td>\n",
       "      <td>0.891586</td>\n",
       "      <td>-2.369334</td>\n",
       "      <td>10673.51</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491226</th>\n",
       "      <td>491226</td>\n",
       "      <td>-0.295538</td>\n",
       "      <td>-0.924505</td>\n",
       "      <td>0.023364</td>\n",
       "      <td>1.059566</td>\n",
       "      <td>0.280535</td>\n",
       "      <td>0.693233</td>\n",
       "      <td>0.976836</td>\n",
       "      <td>-0.311143</td>\n",
       "      <td>0.185700</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.217292</td>\n",
       "      <td>-0.157562</td>\n",
       "      <td>0.445011</td>\n",
       "      <td>1.974289</td>\n",
       "      <td>-0.962623</td>\n",
       "      <td>0.163325</td>\n",
       "      <td>0.130430</td>\n",
       "      <td>-1.861842</td>\n",
       "      <td>9255.71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318639</th>\n",
       "      <td>318639</td>\n",
       "      <td>-0.984914</td>\n",
       "      <td>0.414957</td>\n",
       "      <td>-0.552622</td>\n",
       "      <td>-0.247136</td>\n",
       "      <td>0.166065</td>\n",
       "      <td>0.791129</td>\n",
       "      <td>-0.656226</td>\n",
       "      <td>-1.566631</td>\n",
       "      <td>0.617445</td>\n",
       "      <td>...</td>\n",
       "      <td>2.680002</td>\n",
       "      <td>-2.183399</td>\n",
       "      <td>0.377818</td>\n",
       "      <td>0.200263</td>\n",
       "      <td>0.485012</td>\n",
       "      <td>1.074103</td>\n",
       "      <td>-0.009749</td>\n",
       "      <td>1.203711</td>\n",
       "      <td>18211.81</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446685</th>\n",
       "      <td>446685</td>\n",
       "      <td>-0.909071</td>\n",
       "      <td>0.517082</td>\n",
       "      <td>-0.679894</td>\n",
       "      <td>0.699931</td>\n",
       "      <td>-0.721859</td>\n",
       "      <td>-0.724758</td>\n",
       "      <td>-0.674349</td>\n",
       "      <td>0.364500</td>\n",
       "      <td>-0.882138</td>\n",
       "      <td>...</td>\n",
       "      <td>0.345441</td>\n",
       "      <td>0.210109</td>\n",
       "      <td>-0.422754</td>\n",
       "      <td>0.349922</td>\n",
       "      <td>-0.064643</td>\n",
       "      <td>-0.521351</td>\n",
       "      <td>0.072704</td>\n",
       "      <td>1.840916</td>\n",
       "      <td>2454.39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490279</th>\n",
       "      <td>490279</td>\n",
       "      <td>1.005584</td>\n",
       "      <td>0.295857</td>\n",
       "      <td>-0.645788</td>\n",
       "      <td>1.023021</td>\n",
       "      <td>0.719596</td>\n",
       "      <td>-0.329439</td>\n",
       "      <td>0.094332</td>\n",
       "      <td>-0.095545</td>\n",
       "      <td>-0.670240</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010355</td>\n",
       "      <td>-0.353939</td>\n",
       "      <td>-0.137286</td>\n",
       "      <td>-0.206871</td>\n",
       "      <td>0.741441</td>\n",
       "      <td>0.388425</td>\n",
       "      <td>0.359051</td>\n",
       "      <td>0.560608</td>\n",
       "      <td>9943.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345037</th>\n",
       "      <td>345037</td>\n",
       "      <td>-1.138970</td>\n",
       "      <td>1.070851</td>\n",
       "      <td>-1.203811</td>\n",
       "      <td>0.729509</td>\n",
       "      <td>-1.121804</td>\n",
       "      <td>-1.404404</td>\n",
       "      <td>-0.899363</td>\n",
       "      <td>1.067754</td>\n",
       "      <td>-0.695637</td>\n",
       "      <td>...</td>\n",
       "      <td>0.263305</td>\n",
       "      <td>-0.476927</td>\n",
       "      <td>0.069326</td>\n",
       "      <td>-0.006757</td>\n",
       "      <td>0.528282</td>\n",
       "      <td>-0.587281</td>\n",
       "      <td>1.111546</td>\n",
       "      <td>0.705665</td>\n",
       "      <td>11922.76</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306289</th>\n",
       "      <td>306289</td>\n",
       "      <td>-0.481239</td>\n",
       "      <td>1.069196</td>\n",
       "      <td>-0.963115</td>\n",
       "      <td>1.262110</td>\n",
       "      <td>0.254864</td>\n",
       "      <td>-1.582914</td>\n",
       "      <td>-0.294381</td>\n",
       "      <td>0.259746</td>\n",
       "      <td>-1.126491</td>\n",
       "      <td>...</td>\n",
       "      <td>0.163664</td>\n",
       "      <td>-0.453683</td>\n",
       "      <td>-0.431971</td>\n",
       "      <td>-0.428562</td>\n",
       "      <td>1.712918</td>\n",
       "      <td>0.934449</td>\n",
       "      <td>0.307238</td>\n",
       "      <td>0.683768</td>\n",
       "      <td>22772.46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515502</th>\n",
       "      <td>515502</td>\n",
       "      <td>-0.841052</td>\n",
       "      <td>0.348399</td>\n",
       "      <td>-0.732623</td>\n",
       "      <td>0.553494</td>\n",
       "      <td>-0.950673</td>\n",
       "      <td>0.288186</td>\n",
       "      <td>-0.519068</td>\n",
       "      <td>0.386787</td>\n",
       "      <td>-0.730611</td>\n",
       "      <td>...</td>\n",
       "      <td>0.269629</td>\n",
       "      <td>0.484955</td>\n",
       "      <td>0.175233</td>\n",
       "      <td>-0.612080</td>\n",
       "      <td>0.180018</td>\n",
       "      <td>0.300356</td>\n",
       "      <td>-0.372990</td>\n",
       "      <td>-0.322921</td>\n",
       "      <td>21432.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167696</th>\n",
       "      <td>167696</td>\n",
       "      <td>-0.110155</td>\n",
       "      <td>-0.365570</td>\n",
       "      <td>1.450943</td>\n",
       "      <td>-1.065077</td>\n",
       "      <td>0.656764</td>\n",
       "      <td>1.279073</td>\n",
       "      <td>0.385734</td>\n",
       "      <td>0.013016</td>\n",
       "      <td>0.768832</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028573</td>\n",
       "      <td>1.262582</td>\n",
       "      <td>-0.187531</td>\n",
       "      <td>-2.211880</td>\n",
       "      <td>-0.672890</td>\n",
       "      <td>1.572368</td>\n",
       "      <td>0.262641</td>\n",
       "      <td>0.341398</td>\n",
       "      <td>15385.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171299</th>\n",
       "      <td>171299</td>\n",
       "      <td>-0.034042</td>\n",
       "      <td>-0.537490</td>\n",
       "      <td>0.547181</td>\n",
       "      <td>-0.906487</td>\n",
       "      <td>0.715446</td>\n",
       "      <td>0.707606</td>\n",
       "      <td>0.783780</td>\n",
       "      <td>-0.197858</td>\n",
       "      <td>0.741029</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.082444</td>\n",
       "      <td>0.425324</td>\n",
       "      <td>-0.284130</td>\n",
       "      <td>-2.250032</td>\n",
       "      <td>-1.113239</td>\n",
       "      <td>-0.840946</td>\n",
       "      <td>-0.133923</td>\n",
       "      <td>-0.321674</td>\n",
       "      <td>20118.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>568630 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id        V1        V2        V3        V4        V5        V6  \\\n",
       "529137  529137 -0.928640 -0.238530 -0.757097  0.020714  0.117750  0.200009   \n",
       "491226  491226 -0.295538 -0.924505  0.023364  1.059566  0.280535  0.693233   \n",
       "318639  318639 -0.984914  0.414957 -0.552622 -0.247136  0.166065  0.791129   \n",
       "446685  446685 -0.909071  0.517082 -0.679894  0.699931 -0.721859 -0.724758   \n",
       "490279  490279  1.005584  0.295857 -0.645788  1.023021  0.719596 -0.329439   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "345037  345037 -1.138970  1.070851 -1.203811  0.729509 -1.121804 -1.404404   \n",
       "306289  306289 -0.481239  1.069196 -0.963115  1.262110  0.254864 -1.582914   \n",
       "515502  515502 -0.841052  0.348399 -0.732623  0.553494 -0.950673  0.288186   \n",
       "167696  167696 -0.110155 -0.365570  1.450943 -1.065077  0.656764  1.279073   \n",
       "171299  171299 -0.034042 -0.537490  0.547181 -0.906487  0.715446  0.707606   \n",
       "\n",
       "              V7        V8        V9  ...       V21       V22       V23  \\\n",
       "529137 -0.525105 -0.294222 -0.816412  ... -0.226424  1.428466 -0.231378   \n",
       "491226  0.976836 -0.311143  0.185700  ... -0.217292 -0.157562  0.445011   \n",
       "318639 -0.656226 -1.566631  0.617445  ...  2.680002 -2.183399  0.377818   \n",
       "446685 -0.674349  0.364500 -0.882138  ...  0.345441  0.210109 -0.422754   \n",
       "490279  0.094332 -0.095545 -0.670240  ... -0.010355 -0.353939 -0.137286   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "345037 -0.899363  1.067754 -0.695637  ...  0.263305 -0.476927  0.069326   \n",
       "306289 -0.294381  0.259746 -1.126491  ...  0.163664 -0.453683 -0.431971   \n",
       "515502 -0.519068  0.386787 -0.730611  ...  0.269629  0.484955  0.175233   \n",
       "167696  0.385734  0.013016  0.768832  ...  0.028573  1.262582 -0.187531   \n",
       "171299  0.783780 -0.197858  0.741029  ... -0.082444  0.425324 -0.284130   \n",
       "\n",
       "             V24       V25       V26       V27       V28    Amount  Class  \n",
       "529137 -0.429209  1.398836  1.700747  0.891586 -2.369334  10673.51      1  \n",
       "491226  1.974289 -0.962623  0.163325  0.130430 -1.861842   9255.71      1  \n",
       "318639  0.200263  0.485012  1.074103 -0.009749  1.203711  18211.81      1  \n",
       "446685  0.349922 -0.064643 -0.521351  0.072704  1.840916   2454.39      1  \n",
       "490279 -0.206871  0.741441  0.388425  0.359051  0.560608   9943.88      1  \n",
       "...          ...       ...       ...       ...       ...       ...    ...  \n",
       "345037 -0.006757  0.528282 -0.587281  1.111546  0.705665  11922.76      1  \n",
       "306289 -0.428562  1.712918  0.934449  0.307238  0.683768  22772.46      1  \n",
       "515502 -0.612080  0.180018  0.300356 -0.372990 -0.322921  21432.67      1  \n",
       "167696 -2.211880 -0.672890  1.572368  0.262641  0.341398  15385.03      0  \n",
       "171299 -2.250032 -1.113239 -0.840946 -0.133923 -0.321674  20118.79      0  \n",
       "\n",
       "[568630 rows x 31 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomly shuffling\n",
    "data_updated = data_updated.sample(frac=1)\n",
    "data_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20995844-7fb9-4896-a150-a41d95564a9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "1    284315\n",
       "0    284315\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_updated['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ce7c74-065d-4a07-b859-eb29ba3a0176",
   "metadata": {},
   "source": [
    "### Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8767b03e-331f-41dc-8c89-3c7a55d3e91a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = data_updated.iloc[:250000,:-1]\n",
    "Y = data_updated.iloc[:250000,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e876e36-ec35-41bd-ad18-206b9b794277",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.35, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dec28249-69cd-4fa7-a4d4-d66db792b7df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train : (162500, 30)\n",
      "Shape of X_test : (87500, 30)\n",
      "Shape of Y_train : (162500,)\n",
      "Shape of Y_test : (87500,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of X_train : {X_train.shape}\")\n",
    "print(f\"Shape of X_test : {X_test.shape}\")\n",
    "print(f\"Shape of Y_train : {Y_train.shape}\")\n",
    "print(f\"Shape of Y_test : {Y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af88c666-bf58-445a-a3fc-c5e7aa20003a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cae629",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00857cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances from the trained model\n",
    "feature_importances = rf_classifier.feature_importances_\n",
    "\n",
    "# Create a DataFrame to display feature importances\n",
    "feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Random Forest Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ceba67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "# Use SelectFromModel to automatically select features based on importance\n",
    "sfm = SelectFromModel(clf)\n",
    "sfm.fit(X_train, Y_train)\n",
    "\n",
    "# Get selected features\n",
    "selected_features = X_train.columns[sfm.get_support()]\n",
    "\n",
    "# Transform the data to keep only selected features\n",
    "X_train_selected = sfm.transform(X_train)\n",
    "X_test_selected = sfm.transform(X_test)\n",
    "\n",
    "# Print selected features\n",
    "print(\"Selected Features:\", selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6207cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d0a538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2382d6c6-433b-4ecf-a81b-55a47c5a7abd",
   "metadata": {},
   "source": [
    "## Models and Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3613026-4c13-4f54-b84b-c1ced4f0738d",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "200c73c3-a3d3-42cf-be47-e88b75baf540",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=0.0001)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.0001)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=0.0001)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_r = LogisticRegression(penalty='l2', C=0.0001)\n",
    "logistic_r.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b99f1fc-249e-45d5-9ba4-9bfd19d5b1f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_logistic_r = logistic_r.predict(X_test)\n",
    "Y_logistic_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce75fc4a-9195-41f1-96d9-94a485f524f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99888676 0.99913318 0.99913329 0.99888628 0.99925752]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Assuming you have X_train and Y_train\n",
    "logistic_r = LogisticRegression(penalty='l2', C=0.1)\n",
    "cv_scores_precision = cross_val_score(logistic_r, X_train, Y_train, cv=5, scoring='precision')\n",
    "print(cv_scores_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3d90c08-b818-48e9-aa6c-e3384d4a0dcb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 97.74514285714285%\n",
      "Confusion Matrix : \n",
      " [[43545    43]\n",
      " [ 1930 41982]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy : {accuracy_score(Y_test, Y_logistic_r)*100}%\")\n",
    "print(f\"Confusion Matrix : \\n {confusion_matrix(Y_test, Y_logistic_r)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1458b27e-e7b7-46a5-9d2e-7ddf6ef37481",
   "metadata": {},
   "source": [
    "### 2.Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1353bddd-dfab-4efd-8583-3e82acf98dee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a decision tree classifier with initial regularization\n",
    "decision_tree_classification = DecisionTreeClassifier(\n",
    "    random_state=42,\n",
    "    max_depth=5,  # Start with a shallower tree\n",
    "    min_samples_split=100,  # Require more samples to split\n",
    "    min_samples_leaf=30,  # Require more samples at leaf nodes\n",
    "    max_features='sqrt',  # Consider fewer features at each split\n",
    "    ccp_alpha=0.001  # Enable cost complexity pruning\n",
    ")\n",
    "\n",
    "# Define a grid of hyperparameters to explore\n",
    "param_grid = {\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [50, 100, 150],\n",
    "    'min_samples_leaf': [20, 30, 40],\n",
    "    'ccp_alpha': [0.001, 0.006 , 0.01],\n",
    "}\n",
    "\n",
    "# Perform grid search cross-validation to find optimal hyperparameters\n",
    "grid_search = GridSearchCV(decision_tree_classification, param_grid, cv=5)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Retrieve the best model\n",
    "best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a406643d-4a82-439a-9ef3-b2c58099bd9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Y_DTc= best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "beea5897-757a-4b8d-bb7d-7feda9d36dcc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 99.50742857142856%\n",
      "Confusion Matrix : \n",
      " [[43253   335]\n",
      " [   96 43816]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy : {accuracy_score(Y_test, Y_DTc)*100}%\")\n",
    "print(f\"Confusion Matrix : \\n {confusion_matrix(Y_test, Y_DTc)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e76016-fae8-4be2-b2ee-9d048f16eeac",
   "metadata": {},
   "source": [
    "### 3. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "02a26b94-ca9b-4f75-b548-6389f657901d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_estimators=20)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_estimators=20)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(n_estimators=20)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest_classification = RandomForestClassifier(n_estimators = 20,)\n",
    "random_forest_classification.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3233f5fb-6ba2-4f24-9561-88afcbb07fc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Y_RF_class = random_forest_classification.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e2c41ce7-439d-4b1f-9614-d38cf47fa8b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 99.97142857142856%\n",
      "Confusion Matrix : \n",
      " [[43581     7]\n",
      " [   18 43894]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy : {accuracy_score(Y_test, Y_RF_class)*100}%\")\n",
    "print(f\"Confusion Matrix : \\n {confusion_matrix(Y_test, Y_RF_class)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829acdb4-b8f2-48de-a555-2279b83301fa",
   "metadata": {},
   "source": [
    "### 4. Neural Network approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9386b21e-5dcb-4b14-b610-84c5afc97779",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Software\\New folder\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(X_train.shape[1]))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8671b8b5-761b-4a9e-ad80-65e3aeb64b74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(loss = binary_crossentropy,\n",
    "              optimizer = SGD(learning_rate = 0.001),\n",
    "              metrics = [\n",
    "                  BinaryAccuracy(name='accuracy'),\n",
    "                  Precision(name='precision'),\n",
    "                  Recall(name='recall')]\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7b5c75f1-a88e-436d-b5a0-621fd0182202",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                1984      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               33024     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 16)                4112      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 47457 (185.38 KB)\n",
      "Trainable params: 47457 (185.38 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fbb6550c-d1d1-4712-86a9-5a27bf8713cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From D:\\Software\\New folder\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "3809/3809 [==============================] - 23s 6ms/step - loss: 0.6062 - accuracy: 0.6528 - precision: 0.6326 - recall: 0.7262 - val_loss: 0.3728 - val_accuracy: 0.9101 - val_precision: 0.9975 - val_recall: 0.8211\n",
      "Epoch 2/10\n",
      "3809/3809 [==============================] - 20s 5ms/step - loss: 0.3521 - accuracy: 0.8637 - precision: 0.8889 - recall: 0.8308 - val_loss: 0.1581 - val_accuracy: 0.9483 - val_precision: 0.9979 - val_recall: 0.8978\n",
      "Epoch 3/10\n",
      "3809/3809 [==============================] - 20s 5ms/step - loss: 0.2227 - accuracy: 0.9264 - precision: 0.9574 - recall: 0.8923 - val_loss: 0.0955 - val_accuracy: 0.9654 - val_precision: 0.9976 - val_recall: 0.9325\n",
      "Epoch 4/10\n",
      "3809/3809 [==============================] - 20s 5ms/step - loss: 0.1633 - accuracy: 0.9508 - precision: 0.9748 - recall: 0.9253 - val_loss: 0.0651 - val_accuracy: 0.9763 - val_precision: 0.9969 - val_recall: 0.9553\n",
      "Epoch 5/10\n",
      "3809/3809 [==============================] - 20s 5ms/step - loss: 0.1280 - accuracy: 0.9613 - precision: 0.9788 - recall: 0.9429 - val_loss: 0.0495 - val_accuracy: 0.9819 - val_precision: 0.9971 - val_recall: 0.9662\n",
      "Epoch 6/10\n",
      "3809/3809 [==============================] - 20s 5ms/step - loss: 0.1052 - accuracy: 0.9698 - precision: 0.9837 - recall: 0.9554 - val_loss: 0.0384 - val_accuracy: 0.9861 - val_precision: 0.9975 - val_recall: 0.9745\n",
      "Epoch 7/10\n",
      "3809/3809 [==============================] - 20s 5ms/step - loss: 0.0924 - accuracy: 0.9737 - precision: 0.9854 - recall: 0.9615 - val_loss: 0.0313 - val_accuracy: 0.9889 - val_precision: 0.9978 - val_recall: 0.9799\n",
      "Epoch 8/10\n",
      "3809/3809 [==============================] - 20s 5ms/step - loss: 0.0821 - accuracy: 0.9778 - precision: 0.9875 - recall: 0.9678 - val_loss: 0.0263 - val_accuracy: 0.9909 - val_precision: 0.9981 - val_recall: 0.9836\n",
      "Epoch 9/10\n",
      "3809/3809 [==============================] - 20s 5ms/step - loss: 0.0718 - accuracy: 0.9798 - precision: 0.9884 - recall: 0.9710 - val_loss: 0.0227 - val_accuracy: 0.9921 - val_precision: 0.9983 - val_recall: 0.9859\n",
      "Epoch 10/10\n",
      "3809/3809 [==============================] - 20s 5ms/step - loss: 0.0641 - accuracy: 0.9822 - precision: 0.9897 - recall: 0.9744 - val_loss: 0.0196 - val_accuracy: 0.9931 - val_precision: 0.9983 - val_recall: 0.9877\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, epochs = 10,  validation_split = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "51bbd364-3bd8-49d3-a3e5-05cfc4e026b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# i = np.arange(1, 151)\n",
    "\n",
    "# fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n",
    "# fig.set_figheight(12)\n",
    "# fig.set_figwidth(18)\n",
    "\n",
    "# ax1.plot(i, history.history['loss'])\n",
    "# ax1.plot(i, history.history['val_loss'])\n",
    "# ax1.set_xlabel('Epoch')\n",
    "# ax1.set_ylabel('Loss')\n",
    "# ax1.set_title('Loss vs Epoch')\n",
    "# ax1.legend(['Training', 'Validation'])\n",
    "\n",
    "# ax2.plot(i, history.history['accuracy'])\n",
    "# ax2.plot(i, history.history['val_accuracy'])\n",
    "# ax2.set_xlabel('Epoch')\n",
    "# ax2.set_ylabel('Accuracy')\n",
    "# ax2.set_title('Accuracy vs Epoch')\n",
    "# ax2.legend(['Training', 'Validation'])\n",
    "\n",
    "# # If you want to plot precision and recall, you may need to calculate them during training and store in 'history'\n",
    "# # Assuming history contains 'precision', 'val_precision', 'recall', 'val_recall'\n",
    "# ax3.plot(i, history.history['precision'])\n",
    "# ax3.plot(i, history.history['val_precision'])\n",
    "# ax3.set_xlabel('Epoch')\n",
    "# ax3.set_ylabel('Precision')\n",
    "# ax3.set_title('Precision vs Epoch')\n",
    "# ax3.legend(['Training', 'Validation'])\n",
    "\n",
    "# ax4.plot(i, history.history['recall'])\n",
    "# ax4.plot(i, history.history['val_recall'])\n",
    "# ax4.set_xlabel('Epoch')\n",
    "# ax4.set_ylabel('Recall')\n",
    "# ax4.set_title('Recall vs Epoch')\n",
    "# ax4.legend(['Training', 'Validation'])\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "50519a33-837f-4370-b20e-82a91e4cf19c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2735/2735 [==============================] - 6s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "Y_neural_net = model.predict(X_test)\n",
    "Y_neural_net = [1 if i > 0.5 else 0 for i in list(Y_neural_net)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "99dde1e7-f6c2-4d8b-bab1-07ed4b8f4215",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 99.264%\n",
      "Confusion Matrix : \n",
      " [[43509    79]\n",
      " [  565 43347]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy : {accuracy_score(Y_test, Y_neural_net)*100}%\")\n",
    "print(f\"Confusion Matrix : \\n {confusion_matrix(Y_test, Y_neural_net)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6801c92a-7123-4c66-9240-6c8246cc2bf0",
   "metadata": {},
   "source": [
    "## Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f218a5-397a-44d8-b544-e70e97dabdb8",
   "metadata": {},
   "source": [
    "All models work good but Neural Network gives the highest accuracy marginally i.e. 96.47%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1b757e81-d49d-4b4e-bd89-e6542fa1cdc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Logistic Regression : 97.74514285714285%\n",
      "Accuracy by Decision Tree Classifier : 99.50742857142856%\n",
      "Accuracy by Random Forest Classifier : 99.97142857142856%\n",
      "Accuracy by Neural Network : 99.264%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy by Logistic Regression : {accuracy_score(Y_test, Y_logistic_r)*100}%\")\n",
    "print(f\"Accuracy by Decision Tree Classifier : {accuracy_score(Y_test, Y_DTc)*100}%\")\n",
    "print(f\"Accuracy by Random Forest Classifier : {accuracy_score(Y_test, Y_RF_class)*100}%\")\n",
    "print(f\"Accuracy by Neural Network : {accuracy_score(Y_test, Y_neural_net)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "12c238c6-8365-41a6-8ea8-1448a89a060f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Logistic Regression : \n",
      " [[43545    43]\n",
      " [ 1930 41982]]\n",
      "Confusion Matrix for Decision Tree Classifier : \n",
      " [[43253   335]\n",
      " [   96 43816]]\n",
      "Confusion Matrix for Random Forest Classifier : \n",
      " [[43581     7]\n",
      " [   18 43894]]\n",
      "Confusion Matrix for Neural Network : \n",
      " [[43509    79]\n",
      " [  565 43347]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Confusion Matrix for Logistic Regression : \\n {confusion_matrix(Y_test, Y_logistic_r)}\")\n",
    "print(f\"Confusion Matrix for Decision Tree Classifier : \\n {confusion_matrix(Y_test, Y_DTc)}\")\n",
    "print(f\"Confusion Matrix for Random Forest Classifier : \\n {confusion_matrix(Y_test, Y_RF_class)}\")\n",
    "print(f\"Confusion Matrix for Neural Network : \\n {confusion_matrix(Y_test, Y_neural_net)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f6a3830b-8501-4b37-b52f-0bb45a6a806b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.9770413209676856\n",
      "F1 Score: 0.9951057765463361\n",
      "F1 Score: 0.9997153041121475\n",
      "F1 Score: 0.992626348210401\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1 = f1_score(Y_test, Y_logistic_r)\n",
    "\n",
    "print(f\"F1 Score: {f1}\")\n",
    "############################################\n",
    "f1 = f1_score(Y_test, Y_DTc)\n",
    "\n",
    "print(f\"F1 Score: {f1}\")\n",
    "############################################\n",
    "f1 = f1_score(Y_test, Y_RF_class)\n",
    "\n",
    "print(f\"F1 Score: {f1}\")\n",
    "############################################\n",
    "f1 = f1_score(Y_test, Y_neural_net)\n",
    "\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9f2776-6506-4b07-a50a-d648ce957b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
